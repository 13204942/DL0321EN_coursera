{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "4.1_resnet18_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2592216dbd6b41bb9a7a2dcfe7572c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d05b7f226794ac8b057dc2beb175174",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b686a10c26e145beb2aeeb5ec459dd55",
              "IPY_MODEL_77db6ee61cbb4e50b8353206f0cab243"
            ]
          }
        },
        "6d05b7f226794ac8b057dc2beb175174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b686a10c26e145beb2aeeb5ec459dd55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02533efc4b4142c486c44023c39bf637",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37469994a64740e7892de59f3d96d28c"
          }
        },
        "77db6ee61cbb4e50b8353206f0cab243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7de3c3bad3c54b8b856eaba53d1eb045",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 91.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1d5514c70ee4018a58010e614d96574"
          }
        },
        "02533efc4b4142c486c44023c39bf637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37469994a64740e7892de59f3d96d28c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7de3c3bad3c54b8b856eaba53d1eb045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1d5514c70ee4018a58010e614d96574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IwWqqnAFH7Z",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2AL_ywZFH7c",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPdpgIQFH7e",
        "colab_type": "text"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlHw6ZEOFH7f",
        "colab_type": "text"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHIbCKPVFH7f",
        "colab_type": "text"
      },
      "source": [
        "<h2>Table of Contents</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emf1ZGtAFH7g",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B7N1hMsFH7i",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHFn5vnZFH7l",
        "colab_type": "text"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC5JXauTFH7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dc412d13-729a-406d-b936-6c3164744669"
      },
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-28 15:43:54--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: â€˜Positive_tensors.zipâ€™\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  20.4MB/s    in 2m 3s   \n",
            "\n",
            "2020-07-28 15:45:57 (20.2 MB/s) - â€˜Positive_tensors.zipâ€™ saved [2598656062/2598656062]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyLLVwNbFH7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q Positive_tensors.zip "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4QVpILyFH7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1a87b44b-4658-467f-e8b8-7d1e8c3dfe60"
      },
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-28 15:49:07--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: â€˜Negative_tensors.zipâ€™\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  19.3MB/s    in 1m 43s  \n",
            "\n",
            "2020-07-28 15:50:50 (19.6 MB/s) - â€˜Negative_tensors.zipâ€™ saved [2111408108/2111408108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOsDN48CFH72",
        "colab_type": "text"
      },
      "source": [
        "We will install torchvision:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW090o1LFH72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dbc4ec2c-1ae1-4c66-c439-b90e44ca7522"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pds7EpacFH76",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErsALcfnFH76",
        "colab_type": "text"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVwsVNFVFH77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09a47834-6fbf-427a-de92-3ca577e0a5b7"
      },
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff9fcf41670>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u7Uom3DFH8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfSEYqAjFH8E",
        "colab_type": "text"
      },
      "source": [
        "<!--Empty Space for separating topics-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLpr-vlhFH8E",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc-sxTqJFH8F",
        "colab_type": "text"
      },
      "source": [
        " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhpWra2FFH8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2d6dda9-719d-478e-aba8-8841b445ac65"
      },
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        #directory=\"/home/dsxuser/work\"\n",
        "        directory=\"/content/\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcnuvq9SFH8K",
        "colab_type": "text"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44DXzsAKFH8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1243e837-fbef-4496-ea7c-8aa368fee634"
      },
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-APIwveFH8P",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXelrqqEFH8P",
        "colab_type": "text"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnSY8s3yFH8R",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HOoMVQUFH8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "2592216dbd6b41bb9a7a2dcfe7572c00",
            "6d05b7f226794ac8b057dc2beb175174",
            "b686a10c26e145beb2aeeb5ec459dd55",
            "77db6ee61cbb4e50b8353206f0cab243",
            "02533efc4b4142c486c44023c39bf637",
            "37469994a64740e7892de59f3d96d28c",
            "7de3c3bad3c54b8b856eaba53d1eb045",
            "f1d5514c70ee4018a58010e614d96574"
          ]
        },
        "outputId": "3e6c7312-6fa1-4cbe-9020-e57311fb0852"
      },
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "# Type your code here"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2592216dbd6b41bb9a7a2dcfe7572c00",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGkQHlavFH8X",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdXm-wnhFH8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "# Type your code here"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9uDuAZiFH8b",
        "colab_type": "text"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0l5Iv7wFH8b",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeY30bpFFH8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZucEtUEnFH8f",
        "colab_type": "text"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob-nvSddFH8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97120da2-8762-4c3d-cd99-3d503d7887e6"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNmrQA4FH8i",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8-Jj5lHFH8j",
        "colab_type": "text"
      },
      "source": [
        "In this question you will train your, model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmp224PsFH8j",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFKNUpeFH8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Type your code here"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISbAXHV_FH8o",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk2GNXJZFH8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=100)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEwaFu3MFH8s",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8804KV9FH8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozSZ3PwFH8x",
        "colab_type": "text"
      },
      "source": [
        "<!--Empty Space for separating topics-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDRLqEy2FH8x",
        "colab_type": "text"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YuTJFnIFH8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()     \n",
        "        #make a prediction\n",
        "        z = model(x)    \n",
        "        # calculate loss\n",
        "        loss = criterion(z, y)     \n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()        \n",
        "        # update parameters\n",
        "        optimizer.step() \n",
        "        \n",
        "        loss_list.append(loss.data)\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()       \n",
        "        #make a prediction\n",
        "        z = model(x_test)         \n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct += (yhat==y_test).sum().item()     \n",
        "   \n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS2W5HcqFH80",
        "colab_type": "text"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVlVQ-M8FH81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36520066-89d1-48aa-c9fa-1045b9f61a0c"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-_ZLXpUFH84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2bd7b727-35e0-4573-9bd4-ec656b076ab2"
      },
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FKGDa5CFH86",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4xBY-kKFH87",
        "colab_type": "text"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9tkq9gaFH88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "46c4dc5b-785f-4d5f-c5a8-48773c514f18"
      },
      "source": [
        "N_runtimes = 0\n",
        "N_queue = 0\n",
        "\n",
        "for x_test, y_test in validation_loader:\n",
        "    # set model to eval \n",
        "    model.eval()       \n",
        "    #make a prediction\n",
        "    z = model(x_test)         \n",
        "    #find max \n",
        "    _, yhat = torch.max(z.data, 1)\n",
        "    \n",
        "    for i in range(len(y_test)):\n",
        "      N_queue += 1\n",
        "      if yhat[i] != y_test[i]:\n",
        "        print(\"sample#: %d - predicted value: %d - actual value: %d\" % (N_queue, yhat[i], y_test[i]))\n",
        "        N_runtimes += 1\n",
        "      \n",
        "      if N_runtimes >= 4:\n",
        "        break\n",
        "\n",
        "    if N_runtimes >= 4:\n",
        "      break\n",
        "\n",
        "print(\"Done\")        "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample#: 18 - predicted value: 1 - actual value: 0\n",
            "sample#: 263 - predicted value: 0 - actual value: 1\n",
            "sample#: 319 - predicted value: 0 - actual value: 1\n",
            "sample#: 352 - predicted value: 1 - actual value: 0\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiLczjvFFH9C",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aZup_hGFH9D",
        "colab_type": "text"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLII4H3uFH9D",
        "colab_type": "text"
      },
      "source": [
        "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
      ]
    }
  ]
}